# ğŸ§  Tally AI Infra â€” Chat-Based Financial Q&A with Local RAG

A minimal, local-first clone of Tally's AI infrastructure. Users ask financial questions in natural language â€” the system retrieves relevant context from uploaded documents and responds using a local LLM (via Ollama) or remote providers.

## ğŸš€ Features

- Natural language financial Q&A
- Retrieval-Augmented Generation (RAG) with LangChain
- Embedding & vector search using PostgreSQL + pgvector
- Multi-provider LLM routing (OpenAI, Ollama, etc.)
- Local LLM integration using [Ollama](https://ollama.com/)
- FastAPI backend with async endpoints
- Document upload & ingestion

---

## ğŸ“ Folder Structure

```

tally-ai-infra/
â”‚
â”œâ”€â”€ app/                        # Main FastAPI app
â”‚   â”œâ”€â”€ main.py                 # FastAPI entrypoint
â”‚   â”œâ”€â”€ api/                    # API routes
â”‚   â”‚   â”œâ”€â”€ routes.py           # /ask and /upload endpoints
â”‚   â”œâ”€â”€ core/                   # Core logic
â”‚   â”‚   â”œâ”€â”€ rag.py              # RAG pipeline (embed, retrieve, generate)
â”‚   â”‚   â”œâ”€â”€ embeddings.py       # Embedding logic (e.g., OpenAI/BGE)
â”‚   â”‚   â”œâ”€â”€ schema.py           # Pydantic schemas
â”‚   â”‚   â”œâ”€â”€ model\_router.py     # Multi-LLM routing (OSS vs OpenAI)
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â””â”€â”€ config.py               # Env var handling, constants
â”‚
â”œâ”€â”€ db/
â”‚   â””â”€â”€ dummy_data_apple.txt    # Dummy data to test Apple 2023's financials.
â”‚
â”œâ”€â”€ models/                     # Optional: LLM hosting setup
â”‚   â””â”€â”€ local\_model\_server.py         # Entrypoint to run local model
â”‚
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ postgres/               # Postgres + pgvector container      (Not yet implemented)
â”‚       â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ scripts/                    # One-off utils
â”‚   â”œâ”€â”€ embed\_documents.py      # CLI tool to embed & store documents
â”‚
â”œâ”€â”€ .env                        # API keys, DB URL
â”œâ”€â”€ docker-compose.yml          # Full stack: FastAPI + Postgres + vLLM     (Not yet)
â”œâ”€â”€ requirements.txt            # Python deps
â”œâ”€â”€ README.md                   # Setup & usage guide
â””â”€â”€ .gitignore

````

---

## âš™ï¸ Local Setup

1. **Install dependencies**
   ```bash
   pip install -r requirements.txt
````

2. **Enable Ollama and run a local model**

   ```bash
   ollama run llama3
   ```

3. **Start the FastAPI server**

   ```bash
   uvicorn app.main:app --reload --port 6000
   ```

---

## ğŸ“¤ API Usage

### POST `/api/upload`

Upload a `.txt` file containing financial data to embed.

```bash
curl -X POST http://localhost:6000/api/upload \
  -F "file=@dummy_data_apple.txt"
```

---

### POST `/api/ask`

Ask a question related to the ingested content.

```bash
curl -X POST http://localhost:6000/api/ask \
  -H "Content-Type: application/json" \
  -d '{"query": "What is the revenue of Apple in 2023?"}'
```

#### Example response

```json
{
  "result": {
    "revenue": "$394.3 billion",
    "net_income": "$99.8 billion",
    "period": "Q4 2023"
  }
}
```

---

## ğŸ“¦ Tech Stack

* **Backend:** FastAPI
* **LLM Runtime:** [Ollama](https://ollama.com)  (Llama3.2:1b)
* **Embedding & Vector DB:** PostgreSQL + pgvector
* **RAG Orchestration:** LangChain
* **Deployment:** Docker (Postgres), Local runtime for LLM

---

## ğŸ“ Roadmap

* [ ] Containerize full stack (FastAPI + Ollama + Postgres)
* [ ] Add support for remote LLM fallback (e.g., OpenAI)
* [ ] Benchmark inference speed & quality (OSS vs GPT-4)
* [ ] Deploy on cloud (EC2 or ECS)

---

## ğŸ§  Inspired By

[Tally](https://asktally.com/) â€” Chat-based business intelligence for financial data

> ğŸ“Œ Currently local-only due to EC2 resource limitations. Full deployment coming soon.
